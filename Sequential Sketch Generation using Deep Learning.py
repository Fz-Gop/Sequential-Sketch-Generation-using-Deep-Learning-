# -*- coding: utf-8 -*-
"""M24CSA027_M24CSA031_M24CSE001.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o9ObZFw5zOGDVf8Bth52zAeeo3SH2T_7

#Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import os
import pickle
from sklearn.model_selection import train_test_split
from collections import defaultdict
from tqdm import tqdm
import os
import pickle
import json
from IPython.display import Image, display
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import numpy as np
# %matplotlib inline

categories = ["sailboat", "airplane", "anvil", "axe", "butterfly", "star", "cactus",
              "fan", "cat", "cloud", "fork", "apple", "lollipop"]

for category in categories:
    os.system(f'gsutil -m cp "gs://quickdraw_dataset/full/raw/{category}.ndjson" .')

"""#Defining Constants"""

MAX_SEQ_LEN = 200
BATCH_SIZE = 64
HIDDEN_SIZE = 512
EMBED_SIZE = 100
LEARNING_RATE = 5e-4
NUM_EPOCHS = 5
SAMPLES_PER_CLASS = 15000

"""#Choosing Device"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""#Processing Strokes"""

dataset_json_dir = "dataset_json"
save_file = "dataset_array.pkl"
dataset_array = []

class_vocab = ["sailboat", "airplane", "anvil", "axe", "butterfly", "star", "cactus",
               "fan", "cat", "cloud", "fork", "apple", "lollipop"]

if not os.path.exists(dataset_json_dir):
    os.makedirs(dataset_json_dir)

for file in class_vocab:
    file_path = f"{file}.ndjson"
    if os.path.exists(file_path):
        os.rename(file_path, os.path.join(dataset_json_dir, file_path))

def parse_line(ndjson_line):
    sample = json.loads(ndjson_line)
    label = sample["word"]
    strokes = sample["drawing"]
    return strokes, label

for file in class_vocab:
    file_path = os.path.join(dataset_json_dir, f"{file}.ndjson")

    if not os.path.exists(file_path):
        print(f" Warning: {file_path} not found! Skipping...")
        continue

    with open(file_path, "r") as f:
        for i in range(SAMPLES_PER_CLASS):
            ndjson_line = f.readline()
            if not ndjson_line:
                break  #
            data = parse_line(ndjson_line)
            dataset_array.append(data)

with open(save_file, "wb") as f:
    pickle.dump(dataset_array, f)

with open(save_file, "rb") as f:
    dataset_array = pickle.load(f)

print(f"Successfully processed {len(dataset_array)} samples and saved to {save_file}")

def to_delta_strokes(strokes):
    points = []
    prev_x, prev_y = 0, 0

    for i, stroke in enumerate(strokes):
        x_list, y_list = stroke
        for j, (x, y) in enumerate(zip(x_list, y_list)):
            dx = x - prev_x
            dy = y - prev_y
            prev_x, prev_y = x, y


            pen_state = 1 if j == len(x_list) - 1 else 0
            points.append([dx, dy, pen_state])


    if points:
        points[-1][2] = 2
    else:
        points.append([0, 0, 2])

    return torch.tensor(np.array(points), dtype=torch.float32)

def delta_to_strokes(delta_strokes):
    if isinstance(delta_strokes, torch.Tensor):
        delta_strokes = delta_strokes.cpu().numpy()

    x, y = 0, 0
    current_stroke_x = []
    current_stroke_y = []
    strokes = []

    for dx, dy, pen in delta_strokes:
        x += dx
        y += dy


        current_stroke_x.append(x)
        current_stroke_y.append(y)


        if pen >= 1:
            if current_stroke_x:
                strokes.append([current_stroke_x.copy(), current_stroke_y.copy()])
                current_stroke_x = []
                current_stroke_y = []

            if pen >= 2:
                break


    if current_stroke_x:
        strokes.append([current_stroke_x, current_stroke_y])


    if not strokes:
        strokes = [[[0], [0]]]

    return strokes

def normalize_strokes(strokes):
    """Normalize stroke coordinates to range [0, 1]."""
    xs, ys = [], []
    for stroke in strokes:
        xs.extend(stroke[0])
        ys.extend(stroke[1])


    if not xs:
        min_x, max_x = 0, 1
    else:
        min_x, max_x = min(xs), max(xs)

    if not ys:
        min_y, max_y = 0, 1
    else:
        min_y, max_y = min(ys), max(ys)


    normalized_drawing = []
    for stroke in strokes:
        norm_xs = [(x - min_x) / (max_x - min_x) if max_x != min_x else 0.0 for x in stroke[0]]
        norm_ys = [(y - min_y) / (max_y - min_y) if max_y != min_y else 0.0 for y in stroke[1]]
        normalized_drawing.append([norm_xs, norm_ys])

    return normalized_drawing

"""#Visualizing Strokes"""

def visualize_strokes(strokes, label):
    plt.figure(figsize=(3, 3))
    for stroke in strokes:
        x, y = stroke[0], stroke[1]
        plt.plot(x, -np.array(y), "k", linewidth=2)
    plt.axis('off')
    plt.title(f'{label}')
    plt.show()

strokes, label = dataset_array[0]
normalized_strokes = normalize_strokes(strokes)
print(normalized_strokes)
visualize_strokes(normalized_strokes, label)

"""#Padding Sequence"""

def pad_sequence(seq, max_seq_len):
    if len(seq) == 0:
        seq = torch.tensor([[0, 0, 0], [0, 0, 2]], dtype=torch.float32)

    if len(seq) >= max_seq_len:
        return seq[:max_seq_len]

    pad_len = max_seq_len - len(seq)
    padding = torch.zeros((pad_len, 3), dtype=torch.float32)
    return torch.cat([seq, padding], dim=0)

"""#Dataset Handling"""

class SketchDataset(Dataset):
    def __init__(self, dataset_array, class_map, max_seq_len=MAX_SEQ_LEN):
        self.samples = []
        self.class_map = class_map
        self.max_seq_len = max_seq_len

        for strokes, class_name in dataset_array:
            normalized_strokes = normalize_strokes(strokes)
            delta_seq = to_delta_strokes(normalized_strokes)
            class_id = class_map.get(class_name.strip().lower(), 0)
            self.samples.append((class_id, delta_seq))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        class_id, delta_seq = self.samples[idx]
        if len(delta_seq) < 2:
            delta_seq = torch.tensor([[0, 0, 0], [0, 0, 2]], dtype=torch.float32)

        delta_seq = pad_sequence(delta_seq, self.max_seq_len)


        mask = (delta_seq[:, 2] != 1).float()
        mask = mask[:-1]


        input_delta = delta_seq[:-1]
        target_delta = delta_seq[1:, :2]
        target_pen = delta_seq[1:, 2].long()

        return class_id, input_delta, target_delta, target_pen, mask

class_map = {file.replace(".ndjson", "").strip().lower(): idx for idx, file in enumerate(class_vocab)}
print("Class map:", class_map)

"""#Splitting Dataset"""

train_array, test_array = train_test_split(dataset_array, test_size = 0.3)
test_array, val_array = train_test_split(test_array, test_size = 0.5)
print(len(train_array), len(test_array), len(val_array))

train_dataset = SketchDataset(train_array, class_map, MAX_SEQ_LEN)
test_dataset = SketchDataset(test_array, class_map, MAX_SEQ_LEN)
val_dataset = SketchDataset(val_array, class_map, MAX_SEQ_LEN)

train_dataLoader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_dataLoader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
val_dataLoader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

"""#Defining Architecture"""

class SketchLSTM(nn.Module):
    def __init__(self, class_vocab_size, class_embed_size, hidden_size, num_layers = 2):
        super(SketchLSTM, self).__init__()
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.class_embedding = nn.Embedding(class_vocab_size, class_embed_size)


        self.input_size = 3 + class_embed_size


        self.lstm = nn.LSTM(self.input_size, hidden_size, batch_first=True, num_layers=self.num_layers)


        self.fc_delta = nn.Linear(self.hidden_size, 2)
        self.fc_pen = nn.Linear(self.hidden_size, 3)
        self.class_head = nn.Linear(self.hidden_size, class_vocab_size)

    def forward(self, stroke_seq, class_id):
        batch_size, seq_len = stroke_seq.size(0), stroke_seq.size(1)


        class_embed = self.class_embedding(class_id).unsqueeze(1).repeat(1, seq_len, 1)


        delta_in = torch.cat([stroke_seq, class_embed], dim=-1)


        lstm_out, _ = self.lstm(delta_in)


        delta_out = self.fc_delta(lstm_out)
        pen_out = self.fc_pen(lstm_out)
        last_hidden = lstm_out[:, -1, :]
        class_out = self.class_head(last_hidden)
        return delta_out, pen_out, class_out

"""#Feedback Tuner - Tuning the weights through feeback from user"""

class FeedbackTuner:
    def __init__(self, num_classes, decay_rate=0.95, boost_value=0.0):
        """
        num_classes: total number of classes used in the dataset
        decay_rate: controls how fast old feedback decays (0.95 = 5% decay per update)
        boost_value: how much to boost weight for feedback classes
        """
        self.num_classes = num_classes
        self.decay_rate = decay_rate
        self.boost_value = boost_value


        self.class_scores = torch.ones(num_classes)

    def update_feedback(self, class_ids):
        """
        class_ids: list of class IDs the user wants to improve (ints)
        """

        if(class_ids != None):

            self.class_scores *= self.decay_rate


            for cls in class_ids:
                self.class_scores[cls] += self.boost_value

    def get_weights(self):
        """
        Returns normalized weights tensor to be used in class loss
        """

        min_val = 0.1
        weights = self.class_scores.clone()
        weights = torch.clamp(weights, min=min_val)


        weights /= weights.mean()
        return weights.detach()

feedback_tuner = FeedbackTuner(len(class_vocab))

"""#Loss Function"""

def sketch_loss(pred_deltas, target_deltas, pred_pen, target_pen, mask,
                pred_class, true_class, lambda_class=1.0, class_weights=None):


    batch_size = pred_deltas.size(0)


    delta_loss = F.mse_loss(
        pred_deltas * mask.unsqueeze(-1),
        target_deltas * mask.unsqueeze(-1),
        reduction='none'
    ).sum(dim=(1, 2))


    pen_loss = F.cross_entropy(
        pred_pen.reshape(-1, 3),
        target_pen.reshape(-1),
        reduction='none'
    ).reshape(batch_size, -1)
    pen_loss = (pen_loss * mask).sum(dim=1)


    class_loss = F.cross_entropy(pred_class, true_class, reduction='none')


    valid_points = mask.sum(dim=1) + 1e-8
    sketch_loss_val = (delta_loss + 2 * pen_loss) / valid_points

    total_loss_per_sample = sketch_loss_val + lambda_class * class_loss

    if class_weights is not None:
        class_weight_values = class_weights[true_class]
        total_loss_per_sample *= class_weight_values


    return total_loss_per_sample.mean()

"""#Improving Model through Feedback Tuner function"""

def improve_model(model, class_ids, train_loader, optimizer, device, num_epochs):
    feedback_tuner.update_feedback(class_ids)

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        batch_count = 0

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}", unit="batch")
        for batch in progress_bar:
            if any(x.numel() == 0 for x in batch):
                continue

            class_id, input_delta, target_delta, target_pen, mask = [item.to(device) for item in batch]
            if mask.sum() == 0:
                continue

            optimizer.zero_grad()
            pred_delta, pred_pen, pred_class = model(input_delta, class_id)

            class_weights = feedback_tuner.get_weights().to(device)
            loss = sketch_loss(pred_delta, target_delta, pred_pen, target_pen,
                               mask, pred_class, class_id,
                               class_weights=class_weights)

            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            batch_count += 1

        train_loss = running_loss / batch_count if batch_count > 0 else 0.0
        print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}")

"""#Evaluation Function"""

def evaluate_model(model, dataloader, device):

    model.eval()
    total_loss = 0.0
    batch_count = 0

    with torch.no_grad():
        for batch in dataloader:
            if any(x.numel() == 0 for x in batch):
                continue
            class_id, input_delta, target_delta, target_pen, mask = [item.to(device) for item in batch]
            if mask.sum() == 0:
                continue

            pred_delta, pred_pen, pred_class = model(input_delta, class_id)
            loss = sketch_loss(pred_delta, target_delta, pred_pen, target_pen, mask, pred_class, class_id)
            total_loss += loss.item()
            batch_count += 1

    if batch_count == 0:
        return 0.0
    return total_loss / batch_count

"""#Training Model

"""

def train_model(model, train_loader, val_loader, optimizer, device, num_epochs, patience=2):
    last_val_loss = float('inf')
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)
    no_improvement = 0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        batch_count = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}", unit="batch")

        for batch in progress_bar:

            if any(x.numel() == 0 for x in batch):
                continue

            class_id, input_delta, target_delta, target_pen, mask = [item.to(device) for item in batch]
            if mask.sum() == 0:
                continue

            optimizer.zero_grad()

            pred_delta, pred_pen, pred_class = model(input_delta, class_id)
            loss = sketch_loss(pred_delta, target_delta, pred_pen, target_pen, mask, pred_class, class_id)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            batch_count += 1

        if batch_count > 0:
            train_loss = running_loss / batch_count
        else:
            train_loss = 0.0


        val_loss = evaluate_model(model, val_loader, device)
        print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

        scheduler.step(val_loss)



        if val_loss >= last_val_loss:
            no_improvement += 1
            if no_improvement >= patience:
                print("Validation loss not decreasing. Stopping early.")
                break

        last_val_loss = val_loss

len(train_dataLoader)

"""#Model Initializations and Training"""

model = SketchLSTM(class_vocab_size=len(class_vocab), class_embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE)
model.load_state_dict(torch.load("sketch_lstm.pth", weights_only=True))
model.to(device)
adam = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
train_model(model=model, train_loader=train_dataLoader, val_loader=val_dataLoader, optimizer=adam, device=device, num_epochs=NUM_EPOCHS)
torch.save(model.state_dict(), "sketch_lstm.pth")

"""#Generating Sketch for example class"""

def generate_sketch(model, class_id, max_len=200):
    model.eval()
    input_seq = torch.tensor([[[0.5, 0.5, 0.0]]], dtype=torch.float32).to(device)
    class_tensor = torch.tensor([class_id], dtype=torch.long).to(device)
    delta_array = []
    for _ in range(max_len):
        with torch.no_grad():
            delta, pen_out, _ = model(input_seq, class_tensor)
        dx, dy = delta[0, -1].cpu().numpy()
        pen_logits = pen_out[0, -1]
        pen_state = torch.argmax(pen_logits).item()
        delta_array.append([dx, dy, pen_state])
        if pen_state == 2:
            break
        next_input_delta = torch.tensor([[[dx, dy, float(pen_state)]]], dtype=torch.float32).to(device)
        input_seq = torch.cat([input_seq, next_input_delta], dim=1)
    return torch.tensor(np.array(delta_array), dtype=torch.float32)

print(class_vocab)

class_id = class_map["axe"]
preds = generate_sketch(model, class_id, 50)

print(preds)
strokes = delta_to_strokes(preds)
print(strokes)
visualize_strokes(strokes, 'Generated sketch')

model_path = os.path.join(os.path.dirname("project"), "sketch_lstm_genrator.pth")
print(model_path)

class_vocab = ["sailboat", "airplane", "anvil", "axe", "butterfly", "star", "cactus"
               , "fan", "cat", "cloud", "fork", "apple", "lollipop"]

model = SketchLSTM(class_vocab_size=len(class_vocab),
                   class_embed_size=EMBED_SIZE,
                   hidden_size=HIDDEN_SIZE)

model.load_state_dict(torch.load("sketch_lstm.pth"))
model.to(device)
model.eval()

class_map= {'sailboat': 0, 'airplane': 1, 'anvil': 2, 'axe': 3, 'butterfly': 4, 'star': 5, 'cactus': 6, 'fan': 7, 'cat': 8, 'cloud': 9, 'fork': 10, 'apple': 11, 'lollipop': 12}

class_id = class_map["apple"]
preds = generate_sketch(model, class_id, 10)

strokes = delta_to_strokes(preds)

print(strokes)

visualize_strokes(strokes, 'Generated sketch')

def seq_by_seq(model, class_id, max_seq_len):
  for i in range(0,max_seq_len,10):
    preds = generate_sketch(model, class_id, i)
    strokes = delta_to_strokes(preds)
    visualize_strokes(strokes, 'Generated sketch')

seq_by_seq(model, class_map["apple"], 50)

import matplotlib
matplotlib.use("Agg")


def animate_stroke_save(stroke, label="Animated Drawing", num_steps=10, interval=500):

    x, y = stroke
    x = np.array(x)
    y = np.array(y)

    indices = np.linspace(1, len(x), num_steps, endpoint=True, dtype=int)


    fig, ax = plt.subplots(figsize=(6, 6))
    ax.set_title(label)
    ax.axis('off')


    margin = 0.1
    ax.set_xlim(np.min(x) - margin, np.max(x) + margin)
    ax.set_ylim(-np.max(y) - margin, -np.min(y) + margin)


    line, = ax.plot([], [], "k", linewidth=2)

    def init():
        line.set_data([], [])
        return (line,)

    def update(frame):

        idx = indices[frame]

        line.set_data(x[:idx], -y[:idx])
        return (line,)


    ani = animation.FuncAnimation(fig, update, frames=len(indices),
                                  init_func=init, blit=True, interval=interval, repeat=False)


    ani.save("animated_stroke.gif", writer="imagemagick")

    plt.close(fig)

animate_stroke_save(strokes[0], label="Animated Drawing", num_steps=10, interval=500)

display(Image(filename="animated_stroke.gif"))